{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "dataset = open('korpus-ver-2.txt', 'r', encoding=\"utf-8\")\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += [\"<judul>\", \"</judul>\", \"<sinopsis>\", \"</sinopsis>\"]\n",
    "\n",
    "lines = dataset.readlines()\n",
    "len_lines = len(lines)\n",
    "counter = 0\n",
    "list_kalimat = []\n",
    "list_label = []\n",
    "\n",
    "for i in range(0,len_lines,3):\n",
    "    whitelist = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "    judul = lines[i].lower()\n",
    "    judul_arr = judul.split()\n",
    "    new_judul_arr = [''.join(filter(whitelist.__contains__, word)) for word in judul_arr if word not in stop_words]\n",
    "#     print(new_judul_arr)\n",
    "    new_judul = \" \".join(new_judul_arr)\n",
    "    #print(new_judul)\n",
    "    sinopsis = lines[i+1].lower()\n",
    "    sinopsis_arr = sinopsis.split()\n",
    "    new_sinopsis_arr = [''.join(filter(whitelist.__contains__, word)) for word in sinopsis_arr if word not in stop_words]\n",
    "    #print(new_sinopsis_arr)\n",
    "    new_sinopsis = \" \".join(new_sinopsis_arr)\n",
    "    #print(new_sinopsis)\n",
    "    \n",
    "    kalimat = new_judul + \" \" + new_sinopsis\n",
    "#     print(kalimat)\n",
    "#     print()\n",
    "    \n",
    "    fakultas_arr = lines[i+2].lower().split()\n",
    "    fakultas = fakultas_arr[1]\n",
    "    #print(fakultas)\n",
    "    \n",
    "    list_kalimat.append(kalimat)\n",
    "    list_label.append(fakultas)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "vectorizer.fit(list_kalimat)\n",
    "\n",
    "X = vectorizer.transform(list_kalimat).toarray()\n",
    "y = list_label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7894736842105263\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Logistic Regression :\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6842105263157895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "score_nb = clf.score(X_test, y_test)\n",
    "print(\"Multinomial Naive Bayes:\", score_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest : 0.631578947368421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "for_clf = RandomForestClassifier()\n",
    "for_clf.fit(X_train, y_train)\n",
    "score_forest = for_clf.score(X_test, y_test)\n",
    "print(\"Random Forest : \" + str(score_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost : 0.47368421052631576\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier()\n",
    "ada_clf.fit(X_train, y_train)\n",
    "score_ada = ada_clf.score(X_test, y_test)\n",
    "print(\"AdaBoost : \" + str(score_ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PERHITUNGAN SEDERHANA DENGAN COUNTER ###\n",
    "### TOTALLY NEED IMPROVEMENT ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO IMPROVEMENT : stemming\n",
    "# Korpus belum lengkap,masih minim\n",
    "dataset = open('korpus.txt', 'r')\n",
    "\n",
    "words = dataset.read().split()\n",
    "\n",
    "#gabung bahasa indonesia/inggris\n",
    "stopwords = [\"in\", \"a\", \"book\", \"write\", \"writer\",\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "rekam_judul = False\n",
    "rekam_sinopsis = False\n",
    "rekam_fakultas = False\n",
    "\n",
    "tmp_judul = []\n",
    "tmp_sinopsis = []\n",
    "tmp_fakultas = []\n",
    "\n",
    "\n",
    "kata_judul = {}\n",
    "kata_sinopsis = {}\n",
    "for i in words:\n",
    "    if(i == '<JUDUL>'):\n",
    "        rekam_judul = True\n",
    "    elif(i == '</JUDUL>'):\n",
    "        rekam_judul = False\n",
    "    elif(i == '<SINOPSIS>'):\n",
    "        rekam_sinopsis = True\n",
    "    elif(i == '</SINOPSIS>'):\n",
    "        rekam_sinopsis = False\n",
    "    elif(i == '<FAKULTAS>'):\n",
    "        rekam_fakultas = True\n",
    "    elif(i == '</FAKULTAS>'):\n",
    "        rekam_fakultas = False\n",
    "        for fakultas in tmp_fakultas:\n",
    "            for judul in tmp_judul:\n",
    "                if(judul in kata_judul):\n",
    "                    if(fakultas in kata_judul[judul]):\n",
    "                        kata_judul[judul][fakultas] += 1\n",
    "                    else:\n",
    "                        kata_judul[judul][fakultas] = 1\n",
    "                else:\n",
    "                    kata_judul[judul] = {}\n",
    "                    kata_judul[judul][fakultas] = 1\n",
    "            for sinopsis in tmp_sinopsis:\n",
    "                if(sinopsis in kata_sinopsis):\n",
    "                    if(fakultas in kata_sinopsis[sinopsis]):\n",
    "                        kata_sinopsis[sinopsis][fakultas] += 1\n",
    "                    else:\n",
    "                        kata_sinopsis[sinopsis][fakultas] = 1\n",
    "                else:\n",
    "                    kata_sinopsis[sinopsis] = {}\n",
    "                    kata_sinopsis[sinopsis][fakultas] = 1\n",
    "                    \n",
    "        tmp_judul.clear()\n",
    "        tmp_sinopsis.clear()\n",
    "        tmp_fakultas.clear()\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        if not(i in stopwords):\n",
    "            \n",
    "            whitelist = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "            word = ''.join(filter(whitelist.__contains__, i))\n",
    "            word = word.lower()\n",
    "            \n",
    "            if len(word) > 1:#katanya masih ada setelah dibersihin, bukan cuma sisa 1 huruf\n",
    "                if rekam_judul:\n",
    "                    tmp_judul.append(word)\n",
    "                elif rekam_sinopsis:\n",
    "                    tmp_sinopsis.append(word)\n",
    "                elif rekam_fakultas:\n",
    "                    word = word.upper()\n",
    "                    tmp_fakultas.append(word)\n",
    "                    \n",
    "                    \n",
    "\n",
    "def predict_class(query):\n",
    "    words = query.split()\n",
    "    bobot_fakultas = {}\n",
    "    bobot_fakultas['FISIP'] = 0\n",
    "    bobot_fakultas['FASILKOM'] = 0\n",
    "    bobot_fakultas['FMIPA'] = 0\n",
    "    bobot_fakultas['FT'] = 0\n",
    "    bobot_fakultas['FK'] = 0\n",
    "    bobot_fakultas['FKG'] = 0\n",
    "    bobot_fakultas['FF'] = 0\n",
    "    bobot_fakultas['FIK'] = 0\n",
    "    bobot_fakultas['FKM'] = 0\n",
    "    bobot_fakultas['FH'] = 0\n",
    "    bobot_fakultas['FIB'] = 0\n",
    "    bobot_fakultas['FEB'] = 0\n",
    "    bobot_fakultas['FIA'] = 0\n",
    "    bobot_fakultas['FPSI'] = 0\n",
    "    \n",
    "    for word in words:\n",
    "        whitelist = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "        word = ''.join(filter(whitelist.__contains__, word))\n",
    "        if len(word) > 0:        \n",
    "            if word not in stopwords :\n",
    "                if word in kata_judul : #kalau gaada ya udah\n",
    "                    for fakultas in kata_judul[word]:\n",
    "                        bobot_fakultas[fakultas] += 2 * kata_judul[word][fakultas]\n",
    "                if word in kata_sinopsis : #kalau gaada ya udah\n",
    "                    for fakultas in kata_sinopsis[word]:\n",
    "                        bobot_fakultas[fakultas] += kata_sinopsis[word][fakultas]\n",
    "\n",
    "    best_bobot = 0\n",
    "    best_fakultas = \"FASILKOM\" #default\n",
    "    \n",
    "    for fakultas in bobot_fakultas:\n",
    "        if bobot_fakultas[fakultas] > best_bobot:\n",
    "            best_bobot = bobot_fakultas[fakultas]\n",
    "            best_fakultas = fakultas\n",
    "            \n",
    "    #print(bobot_fakultas)\n",
    "    return best_fakultas\n",
    "                      \n",
    "# print(tmp_judul)\n",
    "# print()\n",
    "# print(tmp_sinopsis)\n",
    "# print()\n",
    "# print(tmp_fakultas)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input predict class masih berupa kalimat\n",
    "\n",
    "print(predict_class(\"object oriented programming\"))\n",
    "print(predict_class(\"scientific approach for biomolecular\"))\n",
    "print(predict_class(\"cost in financial aspect\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
