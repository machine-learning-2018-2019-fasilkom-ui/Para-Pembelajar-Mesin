{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = open('korpus-ver-2.txt', 'r', encoding=\"utf-8\")\n",
    "lines = dataset.readlines()\n",
    "len_lines = len(lines)\n",
    "counter = 0\n",
    "list_kalimat = []\n",
    "list_rumpun = []\n",
    "list_fakultas = []\n",
    "\n",
    "\n",
    "for i in range(0,len_lines,3):\n",
    "    line_i = lines[i]\n",
    "    judul = line_i\n",
    "    \n",
    "    judul = judul[8:-9] #buang tag <JUDUL> dari kalimat\n",
    "   \n",
    "    sinopsis = lines[i+1]\n",
    "    sinopsis = sinopsis[10:-13] #buang tag <SINOPSIS> dari kalimat\n",
    "   \n",
    "    \n",
    "    kalimat = judul + \" \" + sinopsis\n",
    "\n",
    "\n",
    "    fakultas_arr = lines[i+2].lower().split()\n",
    "    fakultas = fakultas_arr[1]\n",
    "    \n",
    "    \n",
    "    if(fakultas==\"rik\"):\n",
    "        label = \"rik\"\n",
    "    elif(fakultas==\"fmipa\"):\n",
    "        label = \"saintek\"\n",
    "    elif(fakultas==\"fasilkom\"):\n",
    "        label = \"saintek\"\n",
    "    elif(fakultas==\"ft\"):\n",
    "        label = \"saintek\"\n",
    "    elif(fakultas==\"fisip\"):\n",
    "        label = \"soshum\"\n",
    "    elif(fakultas==\"fib\"):\n",
    "        label = \"soshum\"\n",
    "    elif(fakultas==\"fh\"):\n",
    "        label = \"soshum\"\n",
    "    elif(fakultas==\"feb\"):\n",
    "        label = \"soshum\"\n",
    "    elif(fakultas==\"psikologi\"):\n",
    "        label = \"soshum\"\n",
    "    \n",
    "    list_kalimat.append(kalimat)\n",
    "    list_fakultas.append(fakultas)\n",
    "    list_rumpun.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(list_kalimat)\n",
    "list_cleaned = []\n",
    "list_stemmed = []\n",
    "list_lemmatized = []\n",
    "\n",
    "stemmer = stem.snowball.SnowballStemmer(\"english\")\n",
    "stop_words = stopwords.words('english')\n",
    "whitelist = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "\n",
    "for i in range(N):\n",
    "    kalimat = list_kalimat[i]\n",
    "    \n",
    "    #lowercase\n",
    "    kalimat = kalimat.lower()\n",
    "    \n",
    "    #stopwords and tandabaca removal\n",
    "    words = kalimat.split(\" \")\n",
    "    kalimat_clean_arr = [''.join(filter(whitelist.__contains__, word)) for word in words if word not in stop_words]\n",
    "    \n",
    "    #stemming\n",
    "    kalimat_stemmed = ' '.join(stemmer.stem(w) for w in kalimat_clean_arr)\n",
    "    \n",
    "    # TODO lematisasi\n",
    "    kalimat_lemma = kalimat_stemmed\n",
    "    \n",
    "    kalimat_cleaned = ' '.join(kalimat_clean_arr)\n",
    "    \n",
    "    list_cleaned.append(kalimat_cleaned)\n",
    "    list_stemmed.append(kalimat_stemmed)\n",
    "    list_lemmatized.append(kalimat_lemma)    \n",
    "    \n",
    "    \n",
    "# print(list_cleaned[0])\n",
    "# print()\n",
    "# print(list_stemmed[0])\n",
    "# print()\n",
    "# print(list_lemmatized[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer1 = CountVectorizer(min_df=0, lowercase=False)\n",
    "vectorizer1.fit(list_cleaned)\n",
    "\n",
    "X1 = vectorizer1.transform(list_kalimat).toarray()\n",
    "\n",
    "vectorizer2 = CountVectorizer(min_df=0, lazowercase=False)\n",
    "vectorizer2.fit(list_stemmed)\n",
    "\n",
    "X2 = vectorizer2.transform(list_stemmed).toarray()\n",
    "\n",
    "vectorizer3 = CountVectorizer(min_df=0, lowercase=False)\n",
    "vectorizer3.fit(list_lemmatized)\n",
    "\n",
    "X3 = vectorizer3.transform(list_lemmatized).toarray()\n",
    "\n",
    "\n",
    "yf = list_fakultas\n",
    "yr = list_rumpun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
