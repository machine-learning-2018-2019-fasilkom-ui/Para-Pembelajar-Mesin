{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model dengan Fitur N-Gram\n",
    "\n",
    "File ini berisikan program untuk pembangunan model menggunakan fitur n-gram.\n",
    "Prediksi dilakukan terhadap label fakultas dan label rumpun.\n",
    "Eksperimen terhadap fitur bag of words menyangkut preprocessing, metode pembangunan model, dan jumlah label.\n",
    "\n",
    "Harap sebelum menjalankan program, install module yang diperlukan seperti nltk, spacy, dan en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import stem\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = open('korpus-ver-2.txt', 'r', encoding=\"utf-8\")\n",
    "lines = dataset.readlines()\n",
    "len_lines = len(lines)\n",
    "counter = 0\n",
    "list_kalimat = []\n",
    "list_rumpun = []\n",
    "list_fakultas = []\n",
    "\n",
    "\n",
    "for i in range(0,len_lines,3):\n",
    "    line_i = lines[i]\n",
    "    judul = line_i\n",
    "    \n",
    "    judul = judul[8:-10] #buang tag <JUDUL> dari kalimat\n",
    "   \n",
    "    sinopsis = lines[i+1]\n",
    "    sinopsis = sinopsis[10:-13] #buang tag <SINOPSIS> dari kalimat\n",
    "   \n",
    "    \n",
    "    kalimat = judul + \" \" + sinopsis\n",
    "\n",
    "\n",
    "    fakultas_arr = lines[i+2].lower().split()\n",
    "    fakultas = fakultas_arr[1]\n",
    "    \n",
    "    \n",
    "    if(fakultas==\"rik\"):\n",
    "        label = \"rik\"\n",
    "    elif(fakultas==\"fmipa\"):\n",
    "        label = \"saintek\"\n",
    "    elif(fakultas==\"fasilkom\"):\n",
    "        label = \"saintek\"\n",
    "    elif(fakultas==\"ft\"):\n",
    "        label = \"saintek\"\n",
    "    elif(fakultas==\"fisip\"):\n",
    "        label = \"soshum\"\n",
    "    elif(fakultas==\"fib\"):\n",
    "        label = \"soshum\"\n",
    "    elif(fakultas==\"fh\"):\n",
    "        label = \"soshum\"\n",
    "    elif(fakultas==\"feb\"):\n",
    "        label = \"soshum\"\n",
    "    elif(fakultas==\"psikologi\"):\n",
    "        label = \"soshum\"\n",
    "    \n",
    "    list_kalimat.append(kalimat)\n",
    "    list_fakultas.append(fakultas)\n",
    "    list_rumpun.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c0dc6cb8e6cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Load English tokenizer, tagger,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# parser, NER and word vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "N = len(list_kalimat)\n",
    "list_cleaned = []\n",
    "list_stemmed = []\n",
    "list_lemmatized = []\n",
    "\n",
    "stemmer = stem.snowball.SnowballStemmer(\"english\")\n",
    "stop_words = stopwords.words('english')\n",
    "whitelist = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "\n",
    "# Load English tokenizer, tagger,  \n",
    "# parser, NER and word vectors \n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "for i in range(N):\n",
    "    kalimat = list_kalimat[i]\n",
    "    \n",
    "    #lowercase\n",
    "    kalimat = kalimat.lower()\n",
    "    \n",
    "    #stopwords and tandabaca removal\n",
    "    words = kalimat.split(\" \")\n",
    "    kalimat_clean_arr = [''.join(filter(whitelist.__contains__, word)) for word in words if word not in stop_words]\n",
    "    \n",
    "    #stemming\n",
    "    kalimat_stemmed = ' '.join(stemmer.stem(w) for w in kalimat_clean_arr)\n",
    "    \n",
    "    # TODO lematisasi\n",
    "    kalimat_clean_str = ' '.join(kalimat_clean_arr)\n",
    "    spacy_kalimat_clean = nlp(kalimat_clean_str)\n",
    "    kalimat_lemma = ' '.join([w.lemma_ for w in spacy_kalimat_clean])\n",
    "    \n",
    "    kalimat_cleaned = ' '.join(kalimat_clean_arr)\n",
    "    \n",
    "    list_cleaned.append(kalimat_cleaned)\n",
    "    list_stemmed.append(kalimat_stemmed)\n",
    "    list_lemmatized.append(kalimat_lemma)    \n",
    "    \n",
    "    \n",
    "# print(list_cleaned[0])\n",
    "# print()\n",
    "# print(list_stemmed[0])\n",
    "# print()\n",
    "# print(list_lemmatized[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ekstraksi Fitur data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def EkstraksiNgram(tweet):\n",
    "    unigram = CountVectorizer(ngram_range=(1,1), min_df=2, max_features=2000)\n",
    "    unigram.fit(tweet)\n",
    "    unigram_matrix = unigram.transform(tweet).toarray()\n",
    "    bigram = CountVectorizer(ngram_range=(2,2), min_df=2, max_features=2000)\n",
    "    bigram.fit(tweet)\n",
    "    bigram_matrix = bigram.transform(tweet).toarray()\n",
    "    trigram = CountVectorizer(ngram_range=(3,3), min_df=2, max_features=2000)\n",
    "    trigram.fit(tweet)\n",
    "    trigram_matrix = trigram.transform(tweet).toarray()\n",
    "    return unigram_matrix, bigram_matrix, trigram_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-3b1eeb0c61a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEkstraksiNgram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_cleaned\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEkstraksiNgram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_stemmed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX3\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEkstraksiNgram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_lemmatized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-2df0d5aa4202>\u001b[0m in \u001b[0;36mEkstraksiNgram\u001b[1;34m(tweet)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mEkstraksiNgram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0munigram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0munigram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0munigram_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munigram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mbigram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m         \"\"\"\n\u001b[1;32m--> 998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    999\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1032\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    959\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m    962\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "_, X1, _ = EkstraksiNgram(list_cleaned)\n",
    "\n",
    "_, X2, _ = EkstraksiNgram(list_stemmed)\n",
    "\n",
    "_, X3 ,_ = EkstraksiNgram(list_lemmatized)\n",
    "\n",
    "yf = list_fakultas\n",
    "yr = list_rumpun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "\n",
      "[0 0 0 ... 0 0 0]\n",
      "\n",
      "crisis\n",
      "saintek\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ind = [i for i in range(N)]\n",
    "\n",
    "ind_train,ind_test, _ , _ = train_test_split(ind, ind, test_size=0.25, random_state=1000)\n",
    "\n",
    "\n",
    "X_train1 = []\n",
    "X_test1 = []\n",
    "X_train2 = []\n",
    "X_test2 = []\n",
    "X_train3 = []\n",
    "X_test3 = []\n",
    "\n",
    "y_train1 = []\n",
    "y_test1 = []\n",
    "\n",
    "y_train2 = []\n",
    "y_test2 = []\n",
    "\n",
    "\n",
    "for i in ind_train:\n",
    "#     X_train1.append(list_cleaned[i])\n",
    "#     X_train2.append(list_stemmed[i])\n",
    "#     X_train3.append(list_lemmatized[i])\n",
    "    X_train1.append(X1[i])\n",
    "    X_train2.append(X2[i])\n",
    "    X_train3.append(X3[i])\n",
    "    \n",
    "    y_train1.append(yf[i])\n",
    "    y_train2.append(yr[i])\n",
    "    \n",
    "\n",
    "for i in ind_test:\n",
    "    X_test1.append(X1[i])\n",
    "    X_test2.append(X2[i])\n",
    "    X_test3.append(X3[i])\n",
    "    \n",
    "    y_test1.append(yf[i])\n",
    "    y_test2.append(yr[i])\n",
    "    \n",
    "print(X_test1[1])\n",
    "print()\n",
    "print(X_test2[1])\n",
    "print()\n",
    "print(y_test1[1])\n",
    "print(y_test2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pembangunan Model dan Akurasi\n",
    "\n",
    "#### Fungsi untuk confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False,  title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.colorbar()\n",
    "    tick_marks = numpy.arange(len(classes)) \n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\") \n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    #print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): \n",
    "        plt.text(j, i, cm[i, j],  horizontalalignment=\"center\",  color=\"white\" if cm[i, j] > thresh else \"black\") \n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.ylabel('True label') \n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression untuk X1 dan Y1 : 0.3333333333333333\n",
      "Logistic Regression untuk X2 dan Y1 : 0.3717948717948718\n",
      "Logistic Regression untuk X3 dan Y1 : 0.3717948717948718\n",
      "Logistic Regression untuk X1 dan Y2 : 0.8076923076923077\n",
      "Logistic Regression untuk X2 dan Y2 : 0.8076923076923077\n",
      "Logistic Regression untuk X3 dan Y2 : 0.8076923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier_log = LogisticRegression()\n",
    "\n",
    "classifier_log.fit(X_train1, y_train1)\n",
    "predicted = classifier_log.predict(X_test1)\n",
    "score = classifier_log.score(X_test1, y_test1)\n",
    "\n",
    "print(\"Logistic Regression untuk X1 dan Y1 :\", score)\n",
    "\n",
    "# matrix = confusion_matrix(y_test1, predicted)\n",
    "# plot_confusion_matrix(matrix, classes = set(yf), title= 'CM for X1 and Y1')\n",
    "\n",
    "classifier_log.fit(X_train2, y_train1)\n",
    "score = classifier_log.score(X_test2, y_test1)\n",
    "\n",
    "print(\"Logistic Regression untuk X2 dan Y1 :\", score)\n",
    "\n",
    "classifier_log.fit(X_train3, y_train1)\n",
    "score = classifier_log.score(X_test3, y_test1)\n",
    "\n",
    "print(\"Logistic Regression untuk X3 dan Y1 :\", score)\n",
    "\n",
    "\n",
    "classifier_log.fit(X_train1, y_train2)\n",
    "predicted = classifier_log.predict(X_test1)\n",
    "score = classifier_log.score(X_test1, y_test2)\n",
    "\n",
    "print(\"Logistic Regression untuk X1 dan Y2 :\", score)\n",
    "\n",
    "# matrix = confusion_matrix(y_test2, predicted)\n",
    "# plot_confusion_matrix(matrix, classes = set(yr), title= 'CM for X1 and Y2')\n",
    "\n",
    "classifier_log.fit(X_train2, y_train2)\n",
    "score = classifier_log.score(X_test2, y_test2)\n",
    "\n",
    "print(\"Logistic Regression untuk X2 dan Y2 :\", score)\n",
    "\n",
    "classifier_log.fit(X_train3, y_train2)\n",
    "score = classifier_log.score(X_test3, y_test2)\n",
    "\n",
    "print(\"Logistic Regression untuk X3 dan Y2 :\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB untuk X1 dan Y1 : 0.3717948717948718\n",
      "Gaussian NB untuk X2 dan Y1 : 0.3717948717948718\n",
      "Gaussian NB untuk X3 dan Y1 : 0.3717948717948718\n",
      "Gaussian NB untuk X1 dan Y2 : 0.7435897435897436\n",
      "Gaussian NB untuk X2 dan Y2 : 0.7435897435897436\n",
      "Gaussian NB untuk X3 dan Y2 : 0.7435897435897436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier_gnb = GaussianNB()\n",
    "\n",
    "classifier_gnb.fit(X_train1, y_train1)\n",
    "score = classifier_gnb.score(X_test1, y_test1)\n",
    "\n",
    "print(\"Gaussian NB untuk X1 dan Y1 :\", score)\n",
    "\n",
    "classifier_gnb.fit(X_train2, y_train1)\n",
    "score = classifier_gnb.score(X_test2, y_test1)\n",
    "\n",
    "print(\"Gaussian NB untuk X2 dan Y1 :\", score)\n",
    "\n",
    "classifier_gnb.fit(X_train3, y_train1)\n",
    "score = classifier_gnb.score(X_test3, y_test1)\n",
    "\n",
    "print(\"Gaussian NB untuk X3 dan Y1 :\", score)\n",
    "\n",
    "classifier_gnb.fit(X_train1, y_train2)\n",
    "score = classifier_gnb.score(X_test1, y_test2)\n",
    "\n",
    "print(\"Gaussian NB untuk X1 dan Y2 :\", score)\n",
    "\n",
    "classifier_gnb.fit(X_train2, y_train2)\n",
    "score = classifier_gnb.score(X_test2, y_test2)\n",
    "\n",
    "print(\"Gaussian NB untuk X2 dan Y2 :\", score)\n",
    "\n",
    "classifier_gnb.fit(X_train3, y_train2)\n",
    "score = classifier_gnb.score(X_test3, y_test2)\n",
    "\n",
    "print(\"Gaussian NB untuk X3 dan Y2 :\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB untuk X1 dan Y1 : 0.358974358974359\n",
      "Multinomial NB untuk X2 dan Y1 : 0.38461538461538464\n",
      "Multinomial NB untuk X3 dan Y1 : 0.38461538461538464\n",
      "Multinomial NB untuk X1 dan Y2 : 0.7692307692307693\n",
      "Multinomial NB untuk X2 dan Y2 : 0.7435897435897436\n",
      "Multinomial NB untuk X3 dan Y2 : 0.7435897435897436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier_mnb = MultinomialNB()\n",
    "\n",
    "classifier_mnb.fit(X_train1, y_train1)\n",
    "score = classifier_mnb.score(X_test1, y_test1)\n",
    "\n",
    "print(\"Multinomial NB untuk X1 dan Y1 :\", score)\n",
    "\n",
    "classifier_mnb.fit(X_train2, y_train1)\n",
    "score = classifier_mnb.score(X_test2, y_test1)\n",
    "\n",
    "print(\"Multinomial NB untuk X2 dan Y1 :\", score)\n",
    "\n",
    "classifier_mnb.fit(X_train3, y_train1)\n",
    "score = classifier_mnb.score(X_test3, y_test1)\n",
    "\n",
    "print(\"Multinomial NB untuk X3 dan Y1 :\", score)\n",
    "\n",
    "classifier_mnb.fit(X_train1, y_train2)\n",
    "score = classifier_mnb.score(X_test1, y_test2)\n",
    "\n",
    "print(\"Multinomial NB untuk X1 dan Y2 :\", score)\n",
    "\n",
    "classifier_mnb.fit(X_train2, y_train2)\n",
    "score = classifier_mnb.score(X_test2, y_test2)\n",
    "\n",
    "print(\"Multinomial NB untuk X2 dan Y2 :\", score)\n",
    "\n",
    "classifier_mnb.fit(X_train3, y_train2)\n",
    "score = classifier_mnb.score(X_test3, y_test2)\n",
    "\n",
    "print(\"Multinomial NB untuk X3 dan Y2 :\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest untuk X1 dan Y1 : 0.24358974358974358\n",
      "Random Forest untuk X2 dan Y1 : 0.21794871794871795\n",
      "Random Forest untuk X3 dan Y1 : 0.2692307692307692\n",
      "Random Forest untuk X1 dan Y2 : 0.8076923076923077\n",
      "Random Forest untuk X2 dan Y2 : 0.7435897435897436\n",
      "Random Forest untuk X3 dan Y2 : 0.717948717948718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "classifier_rf = RandomForestClassifier()\n",
    "\n",
    "classifier_rf.fit(X_train1, y_train1)\n",
    "score = classifier_rf.score(X_test1, y_test1)\n",
    "\n",
    "print(\"Random Forest untuk X1 dan Y1 :\", score)\n",
    "\n",
    "classifier_rf.fit(X_train2, y_train1)\n",
    "score = classifier_rf.score(X_test2, y_test1)\n",
    "\n",
    "print(\"Random Forest untuk X2 dan Y1 :\", score)\n",
    "\n",
    "classifier_rf.fit(X_train3, y_train1)\n",
    "score = classifier_rf.score(X_test3, y_test1)\n",
    "\n",
    "print(\"Random Forest untuk X3 dan Y1 :\", score)\n",
    "\n",
    "classifier_rf.fit(X_train1, y_train2)\n",
    "score = classifier_rf.score(X_test1, y_test2)\n",
    "\n",
    "print(\"Random Forest untuk X1 dan Y2 :\", score)\n",
    "\n",
    "classifier_rf.fit(X_train2, y_train2)\n",
    "score = classifier_rf.score(X_test2, y_test2)\n",
    "\n",
    "print(\"Random Forest untuk X2 dan Y2 :\", score)\n",
    "\n",
    "classifier_rf.fit(X_train3, y_train2)\n",
    "score = classifier_rf.score(X_test3, y_test2)\n",
    "\n",
    "print(\"Random Forest untuk X3 dan Y2 :\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost untuk X1 dan Y1 : 0.08974358974358974\n",
      "AdaBoost untuk X2 dan Y1 : 0.10256410256410256\n",
      "AdaBoost untuk X3 dan Y1 : 0.10256410256410256\n",
      "AdaBoost untuk X1 dan Y2 : 0.7051282051282052\n",
      "AdaBoost untuk X2 dan Y2 : 0.6923076923076923\n",
      "AdaBoost untuk X3 dan Y2 : 0.6923076923076923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "classifier_ada = AdaBoostClassifier()\n",
    "\n",
    "classifier_ada.fit(X_train1, y_train1)\n",
    "score = classifier_ada.score(X_test1, y_test1)\n",
    "\n",
    "print(\"AdaBoost untuk X1 dan Y1 :\", score)\n",
    "\n",
    "classifier_ada.fit(X_train2, y_train1)\n",
    "score = classifier_ada.score(X_test2, y_test1)\n",
    "\n",
    "print(\"AdaBoost untuk X2 dan Y1 :\", score)\n",
    "\n",
    "classifier_ada.fit(X_train3, y_train1)\n",
    "score = classifier_ada.score(X_test3, y_test1)\n",
    "\n",
    "print(\"AdaBoost untuk X3 dan Y1 :\", score)\n",
    "\n",
    "classifier_ada.fit(X_train1, y_train2)\n",
    "score = classifier_ada.score(X_test1, y_test2)\n",
    "\n",
    "print(\"AdaBoost untuk X1 dan Y2 :\", score)\n",
    "\n",
    "classifier_ada.fit(X_train2, y_train2)\n",
    "score = classifier_ada.score(X_test2, y_test2)\n",
    "\n",
    "print(\"AdaBoost untuk X2 dan Y2 :\", score)\n",
    "\n",
    "classifier_ada.fit(X_train3, y_train2)\n",
    "score = classifier_ada.score(X_test3, y_test2)\n",
    "\n",
    "print(\"AdaBoost untuk X3 dan Y2 :\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree untuk X1 dan Y1 : 0.20512820512820512\n",
      "Decision Tree untuk X2 dan Y1 : 0.1794871794871795\n",
      "Decision Tree untuk X3 dan Y1 : 0.21794871794871795\n",
      "Decision Tree untuk X1 dan Y2 : 0.7435897435897436\n",
      "Decision Tree untuk X2 dan Y2 : 0.7435897435897436\n",
      "Decision Tree untuk X3 dan Y2 : 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier_dt = DecisionTreeClassifier()\n",
    "\n",
    "classifier_dt.fit(X_train1, y_train1)\n",
    "score = classifier_dt.score(X_test1, y_test1)\n",
    "\n",
    "print(\"Decision Tree untuk X1 dan Y1 :\", score)\n",
    "\n",
    "classifier_dt.fit(X_train2, y_train1)\n",
    "score = classifier_dt.score(X_test2, y_test1)\n",
    "\n",
    "print(\"Decision Tree untuk X2 dan Y1 :\", score)\n",
    "\n",
    "classifier_dt.fit(X_train3, y_train1)\n",
    "score = classifier_dt.score(X_test3, y_test1)\n",
    "\n",
    "print(\"Decision Tree untuk X3 dan Y1 :\", score)\n",
    "\n",
    "classifier_dt.fit(X_train1, y_train2)\n",
    "score = classifier_dt.score(X_test1, y_test2)\n",
    "\n",
    "print(\"Decision Tree untuk X1 dan Y2 :\", score)\n",
    "\n",
    "classifier_dt.fit(X_train2, y_train2)\n",
    "score = classifier_dt.score(X_test2, y_test2)\n",
    "\n",
    "print(\"Decision Tree untuk X2 dan Y2 :\", score)\n",
    "\n",
    "classifier_dt.fit(X_train3, y_train2)\n",
    "score = classifier_dt.score(X_test3, y_test2)\n",
    "\n",
    "print(\"Decision Tree untuk X3 dan Y2 :\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
